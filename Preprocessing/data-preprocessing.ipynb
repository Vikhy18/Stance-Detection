{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Vikhy\n",
    "<br>\n",
    "Date: 22 March, 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Replace hashtags\n",
    "4. Replace emoticons with the sentiment\n",
    "5. Remove special characters\n",
    "6. Remove multilple spaces\n",
    "7. Remove stop words\n",
    "8. Correct spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Datasets/SemEval6A.csv\")\n",
    "\n",
    "target = data[\"Target\"]\n",
    "tweets = data[\"Tweet\"]\n",
    "stance = data[\"Stance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessings forg...\n",
      "1       Blessed are the peacemakers, for they shall be...\n",
      "2       I am not conformed to this world. I am transfo...\n",
      "3       Salah should be prayed with #focus and #unders...\n",
      "4       And stay in your houses and do not display you...\n",
      "                              ...                        \n",
      "2809    There's a law protecting unborn eagles, but no...\n",
      "2810    I am 1 in 3... I have had an abortion #Abortio...\n",
      "2811    How dare you say my sexual preference is a cho...\n",
      "2812    Equal rights for those 'born that way', no rig...\n",
      "2813    #POTUS seals his legacy w/ 1/2 doz wins. The #...\n",
      "Name: Tweet, Length: 2814, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove URLs\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_urls(tweet):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', tweet)\n",
    "\n",
    "tweets = tweets.apply(remove_urls)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are quite mistaken. :)   #SemST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove mentions\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"words\")\n",
    "english_dictionary = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    words = tweet.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if (word.startswith(\"@\") or word.startswith(\".@\") or word.startswith(\"/@\")) and word[1:] not in english_dictionary:\n",
    "            words[i] = \"\"\n",
    "        elif (word.startswith(\"@\") or word.startswith(\".@\") or word.startswith(\"/@\")) and word[1:] in english_dictionary:\n",
    "            words[i] = word[1:]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(remove_mentions)\n",
    "print(tweets[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessings forg...\n",
      "1       Blessed are the peacemakers, for they shall be...\n",
      "2       I am not conformed to this world. I am transfo...\n",
      "3       Salah should be prayed with focus and understa...\n",
      "4       And stay in your houses and do not display you...\n",
      "                              ...                        \n",
      "2809    There's a law protecting unborn eagles, but no...\n",
      "2810    I am 1 in 3... I have had an abortion Abortion...\n",
      "2811    How dare you say my sexual preference is a cho...\n",
      "2812    Equal rights for those 'born that way', no rig...\n",
      "2813    POTUS seals his legacy w/ 1/2 doz wins. The GO...\n",
      "Name: Tweet, Length: 2814, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Replace hastags with text\n",
    "\n",
    "def replace_hashtags(tweet):\n",
    "    words = tweet.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if \"#\" in word:\n",
    "            words[i] = word.replace(\"#\", \"\")\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(replace_hashtags)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorching summers for UK by end of the century! Thank Rocks I won't be here to suffer them! happy SemST\n"
     ]
    }
   ],
   "source": [
    "# Replace emoticons with the sentiment\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "emoticon_sentiment = {\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "emoticon_sentiment = {\n",
    "    \"üòÄ\": \" happy \",\n",
    "    \"üòÅ\": \" happy \",\n",
    "    \"üòÇ\": \" laughing \",\n",
    "    \"ü§£\": \" laughing \",\n",
    "    \"üòÉ\": \" happy \",\n",
    "    \"üòÑ\": \" happy \",\n",
    "    \"üòÖ\": \" awkward \",\n",
    "    \"üòÜ\": \" laughing \",\n",
    "    \"üòá\": \" blessed \",\n",
    "    \"üòâ\": \" playful \",\n",
    "    \"üòä\": \" happy \",\n",
    "    \"üòã\": \" tasty \",\n",
    "    \"üòå\": \" peaceful \",\n",
    "    \"üòç\": \" love \",\n",
    "    \"üòé\": \" cool \",\n",
    "    \"üòè\": \" smirking \",\n",
    "    \"üòê\": \" neutral \",\n",
    "    \"üòí\": \" unhappy \",\n",
    "    \"üòì\": \" sad \",\n",
    "    \"üòî\": \" sad \",\n",
    "    \"üòï\": \" confused \",\n",
    "    \"üòñ\": \" unhappy \",\n",
    "    \"üòó\": \" kissing \",\n",
    "    \"üòò\": \" kissing \",\n",
    "    \"üòô\": \" kissing \",\n",
    "    \"üòö\": \" kissing \",\n",
    "    \"üòõ\": \" playful \",\n",
    "    \"üòú\": \" playful \",\n",
    "    \"üòù\": \" playful \",\n",
    "    \"üòû\": \" sad \",\n",
    "    \"üòü\": \" worried \",\n",
    "    \"üò†\": \" angry \",\n",
    "    \"üò°\": \" angry \",\n",
    "    \"üò¢\": \" crying \",\n",
    "    \"üò£\": \" unhappy \",\n",
    "    \"üò§\": \" angry \",\n",
    "    \"üò•\": \" sad \",\n",
    "    \"üò¶\": \" worried \",\n",
    "    \"üòß\": \" worried \",\n",
    "    \"üò®\": \" scared \",\n",
    "    \"üò©\": \" unhappy \",\n",
    "    \"üò™\": \" tired \",\n",
    "    \"üò´\": \" unhappy \",\n",
    "    \"üò≠\": \" crying \",\n",
    "    \"üòÆ\": \" surprised \",\n",
    "    \"üòØ\": \" surprised \",\n",
    "    \"üò∞\": \" worried \",\n",
    "    \"üò±\": \" scared \",\n",
    "    \"üò≤\": \" surprised \",\n",
    "    \"üò≥\": \" shocked \",\n",
    "    \"üò¥\": \" sleepy \",\n",
    "    \"üòµ\": \" shocked \",\n",
    "    \"üò∂\": \" neutral \",\n",
    "    \"ü§¢\": \" nauseated \",\n",
    "    \"ü•µ\": \" hot \",\n",
    "    \"ü•∂\": \" cold \",\n",
    "    \"ü•¥\": \" woozy \",\n",
    "    \"ü•∫\": \" pleading \",\n",
    "    \"ü§´\": \" quiet \",\n",
    "    \"ü§¨\": \" cursing \",\n",
    "    \"üßê\": \" inquiring \",\n",
    "    \"ü§®\": \" skeptical \",\n",
    "    \":)\": \" happy \",\n",
    "    \":-)\": \"happy\",\n",
    "    \";)\": \" wink \",\n",
    "    \";-)\": \" wink \",\n",
    "    \":')\": \" laughing \",\n",
    "    \":/\": \" sad \",\n",
    "    \"^_^\": \" blessed \",\n",
    "    \";)\": \" joyful \",\n",
    "    \":p\": \" tasty \",\n",
    "    \":|]\": \" peaceful \",\n",
    "    \"<3\": \" love \",\n",
    "    \"B-)\": \" cool \",\n",
    "    \":^)\": \" smirking \",\n",
    "    \":|\": \" neutral \",\n",
    "    \":'(\": \" sad \",\n",
    "    \":S\": \" confused \",\n",
    "    \":*\": \" kiss \",\n",
    "    \":P\": \" joyful \",\n",
    "    \";P\": \" joyful \",\n",
    "    \"X-P\": \" joyful \",\n",
    "    \">:-( \": \" sad \",\n",
    "    \":O\": \" surprised \",\n",
    "    \":-O\": \" shocked \",\n",
    "    \"zzz\": \" sleepy \",\n",
    "    \":&\": \" nauseated \",\n",
    "    \":-/\": \" sad \",\n",
    "    \":'( \": \" sad \",\n",
    "    \":-@\": \" cursing \",\n",
    "    \"O_o\": \" shocked \",\n",
    "    \"\\U0001F600\": \" happy \",\n",
    "    \"\\U0001F601\": \" happy \",\n",
    "    \"\\U0001F602\": \" laughing \",\n",
    "    \"\\U0001F923\": \" laughing \",\n",
    "    \"\\U0001F603\": \" happy \",\n",
    "    \"\\U0001F604\": \" happy \",\n",
    "    \"\\U0001F605\": \" awkward \",\n",
    "    \"\\U0001F606\": \" laughing \",\n",
    "    \"\\U0001F607\": \" blessed \",\n",
    "    \"\\U0001F609\": \" playful \",\n",
    "    \"\\U0001F60A\": \" happy \",\n",
    "    \"\\U0001F60B\": \" tasty \",\n",
    "    \"\\U0001F60C\": \" peaceful \",\n",
    "    \"\\U0001F60D\": \" love \",\n",
    "    \"\\U0001F60E\": \" cool \",\n",
    "    \"\\U0001F60F\": \" smirking \",\n",
    "    \"\\U0001F610\": \" neutral \",\n",
    "    \"\\U0001F611\": \" unhappy \",\n",
    "    \"\\U0001F612\": \" sad \",\n",
    "    \"\\U0001F613\": \" sad \",\n",
    "    \"\\U0001F614\": \" confused \",\n",
    "    \"\\U0001F615\": \" unhappy \",\n",
    "    \"\\U0001F617\": \" kissing \",\n",
    "    \"\\U0001F618\": \" kissing \",\n",
    "    \"\\U0001F619\": \" kissing \",\n",
    "    \"\\U0001F61A\": \" kissing \",\n",
    "    \"\\U0001F61B\": \" playful \",\n",
    "    \"\\U0001F61C\": \" playful \",\n",
    "    \"\\U0001F61D\": \" playful \",\n",
    "    \"\\U0001F61E\": \" sad \",\n",
    "    \"\\U0001F61F\": \" worried \",\n",
    "    \"\\U0001F620\": \" angry \",\n",
    "    \"\\U0001F621\": \" angry \",\n",
    "    \"\\U0001F622\": \" crying \",\n",
    "    \"\\U0001F623\": \" unhappy \",\n",
    "    \"\\U0001F624\": \" angry \",\n",
    "    \"\\U0001F625\": \" sad \",\n",
    "    \"\\U0001F626\": \" worried \",\n",
    "    \"\\U0001F627\": \" worried \",\n",
    "    \"\\U0001F628\": \" scared \",\n",
    "    \"\\U0001F629\": \" unhappy \",\n",
    "    \"\\U0001F62A\": \" tired \",\n",
    "    \"\\U0001F62B\": \" unhappy \",\n",
    "    \"\\U0001F62D\": \" crying \",\n",
    "    \"\\U0001F62E\": \" surprised \",\n",
    "    \"\\U0001F62F\": \" surprised \",\n",
    "    \"\\U0001F630\": \" worried \",\n",
    "    \"\\U0001F631\": \" scared \",\n",
    "    \"\\U0001F632\": \" surprised \",\n",
    "    \"\\U0001F633\": \" shocked \",\n",
    "    \"\\U0001F634\": \" sleepy \",\n",
    "    \"\\U0001F635\": \" shocked \",\n",
    "    \"\\U0001F636\": \" neutral \",\n",
    "    \"\\U0001F92E\": \" nauseated \",\n",
    "    \"\\U0001F975\": \" hot \",\n",
    "    \"\\U0001F976\": \" cold \",\n",
    "    \"\\U0001F974\": \" woozy \",\n",
    "    \"\\U0001F97A\": \" pleading \",\n",
    "    \"\\U0001F92B\": \" quiet \",\n",
    "    \"\\U0001F92C\": \" cursing \",\n",
    "    \"\\U0001F9D0\": \" inquiring \"\n",
    "}\n",
    "\n",
    "\n",
    "def replace_emoticons(tweet):\n",
    "    for emoticon, sentiment in emoticon_sentiment.items():\n",
    "        tweet = tweet.replace(emoticon, sentiment)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "tweets = tweets.apply(replace_emoticons)\n",
    "print(tweets[865])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "\n",
    "def remove_special_characters(tweet):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", tweet)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(remove_special_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove multiple spaces\n",
    "\n",
    "def remove_multiple_spaces(tweet):\n",
    "    return re.sub(\"\\s+\", \" \", tweet)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(remove_multiple_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "# <Note: It is not very helpful to remove stopwords considering the models. >\n",
    "\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# def remove_stop_words(tweet):\n",
    "#     words = word_tokenize(tweet)\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# tweets = tweets.apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessings forg...\n",
      "1       Blessed are the peacemakers for they shall be ...\n",
      "2       I am not conformed to this world I am transfor...\n",
      "3       salad should be prayed with focus and understa...\n",
      "4       And stay in your houses and do not display our...\n",
      "                              ...                        \n",
      "2809    Theres a law protecting unborn eagles but not ...\n",
      "2810    I am 1 in 3 I have had an abortion AbortionOnD...\n",
      "2811    How dare you say my sexual preference is a cho...\n",
      "2812    Equal rights for those born that way no rights...\n",
      "2813    POTUS seals his legacy i 12 do wins The got ag...\n",
      "Name: Tweet, Length: 2814, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Correct incorrect spellings\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def correct_spellings(tweet):\n",
    "    words = word_tokenize(tweet)\n",
    "    corrected_words = [sc.correction(word) if sc.correction(word) else word for word in words]\n",
    "    corrected_tweet = \" \".join(corrected_words)\n",
    "    return corrected_tweet\n",
    "\n",
    "sc = SpellChecker()\n",
    "tweets = tweets.apply(correct_spellings)\n",
    "print(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data (Takes a long time to correct spellings)\n",
    "\n",
    "data = pd.concat([target, tweets, stance], axis=1)\n",
    "data.to_csv(\"Datasets/corrected_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessings forg...\n",
      "1       Blessed are the peacemakers for they shall be ...\n",
      "2       I am not conformed to this world I am transfor...\n",
      "3       salad should be prayed with focus and understa...\n",
      "4       And stay in your houses and do not display our...\n",
      "                              ...                        \n",
      "2809    Theres a law protecting unborn eagles but not ...\n",
      "2810    I am 1 in 3 I have had an abortion AbortionOnD...\n",
      "2811    How dare you say my sexual preference is a cho...\n",
      "2812    Equal rights for those born that way no rights...\n",
      "2813    POTUS seals his legacy i 12 do wins The got ag...\n",
      "Name: Tweet, Length: 2814, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read data again from the updated file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Datasets/corrected_tweets.csv\")\n",
    "\n",
    "target = data[\"Target\"]\n",
    "tweets = data[\"Tweet\"]\n",
    "stance = data[\"Stance\"]\n",
    "\n",
    "print(tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "#<Note: Over aggressive pruning of words is observed. >\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "# def stemming(tweet):\n",
    "#     tokens = word_tokenize(tweet)\n",
    "#     stemmer = SnowballStemmer(language=\"english\")\n",
    "#     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "#     return \" \".join(stemmed_tokens)\n",
    "\n",
    "# tweets = tweets.apply(stemming)\n",
    "# print(tweets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessing forgi...\n",
      "1       Blessed are the peacemaker for they shall be c...\n",
      "2       I am not conformed to this world I am transfor...\n",
      "3       salad should be prayed with focus and understa...\n",
      "4       And stay in your house and do not display ours...\n",
      "                              ...                        \n",
      "2809    Theres a law protecting unborn eagle but not h...\n",
      "2810    I am 1 in 3 I have had an abortion AbortionOnD...\n",
      "2811    How dare you say my sexual preference is a cho...\n",
      "2812    Equal right for those born that way no right f...\n",
      "2813    POTUS seal his legacy i 12 do win The got agen...\n",
      "Name: Tweet, Length: 2814, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def lemmatization(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(lemmatization)\n",
    "print(tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([target, tweets, stance], axis=1)\n",
    "data.to_csv(\"Datasets/corrected_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
