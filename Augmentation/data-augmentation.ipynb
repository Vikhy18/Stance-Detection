{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Vikhy\n",
    "<br>\n",
    "Date: 22 March, 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "1. Data Expansion\n",
    "2. Synonym Expansion\n",
    "3. Phrase Expansion\n",
    "4. Query Reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Datasets/corrected_tweets.csv\")\n",
    "\n",
    "target = data[\"Target\"]\n",
    "tweets = data[\"Tweet\"]\n",
    "stance = data[\"Stance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Target  \\\n",
      "0                      Atheism   \n",
      "1                      Atheism   \n",
      "2                      Atheism   \n",
      "3                      Atheism   \n",
      "4                      Atheism   \n",
      "...                        ...   \n",
      "2809  Legalization of Abortion   \n",
      "2810  Legalization of Abortion   \n",
      "2811  Legalization of Abortion   \n",
      "2812  Legalization of Abortion   \n",
      "2813  Legalization of Abortion   \n",
      "\n",
      "                                                  Tweet   Stance  \n",
      "0     dear lord thank u for all of ur blessing forgi...  AGAINST  \n",
      "1     Blessed are the peacemaker for they shall be c...  AGAINST  \n",
      "2     I am not conformed to this world I am transfor...  AGAINST  \n",
      "3     salad should be prayed with focus and understa...  AGAINST  \n",
      "4     And stay in your house and do not display ours...  AGAINST  \n",
      "...                                                 ...      ...  \n",
      "2809  Theres a law protecting unborn eagle but not h...  AGAINST  \n",
      "2810  I am 1 in 3 I have had an abortion AbortionOnD...  AGAINST  \n",
      "2811  How dare you say my sexual preference is a cho...  AGAINST  \n",
      "2812  Equal right for those born that way no right f...  AGAINST  \n",
      "2813  POTUS seal his legacy i 12 do win The got agen...  AGAINST  \n",
      "\n",
      "[2814 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atheism' 'climate modification be A real care'\n",
      " \"women's_liberationist move\" 'Sir_Edmund_Hillary Hilary_Rodham_Clinton'\n",
      " 'legitimation of abortion']\n"
     ]
    }
   ],
   "source": [
    "# Synonym Expansion\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def synonym_expansion(target):\n",
    "    tokens = word_tokenize(target)\n",
    "    extended_tokens = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        synonyms = []\n",
    "        for synonym in wordnet.synsets(token):\n",
    "            for lemma in synonym.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "                if len(synonyms) >= 3:\n",
    "                    break\n",
    "            if len(synonyms) >= 3:\n",
    "                    break\n",
    "        if synonyms:\n",
    "            idx = tokens.index(token)\n",
    "            # extended_tokens = tokens + [synonyms[1]]\n",
    "            tokens[idx] = synonyms[2]\n",
    "            extended_tokens = tokens\n",
    "    return \" \".join(extended_tokens)\n",
    "\n",
    "extended_target = target.apply(synonym_expansion)\n",
    "print(extended_target.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the doctrine or belief that there is no God'\n",
      " 'the weather in some location averaged over some long period of time an event that occurs when something passes from one state or phase to another have the quality of being; (copula, used with an adjective or a predicate noun) a metric unit of length equal to one ten billionth of a meter (or 0.0001 micron); used to specify wavelengths of electromagnetic radiation any rational or irrational number something that interests you because it is important or affects you'\n",
      " 'a supporter of feminism a change of position that does not entail a change of location'\n",
      " 'New Zealand mountaineer who in 1953 first attained the summit of Mount Everest with his Sherpa guide Tenzing Norgay (born in 1919) wife of President Clinton and later a woman member of the United States Senate (1947-)'\n",
      " 'the act of making lawful  termination of pregnancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Phrase Expansion\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def phrase_expansion(target):\n",
    "    tokens = word_tokenize(target)\n",
    "    definitions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        synsets = wordnet.synsets(token)\n",
    "        if synsets:\n",
    "            synset = synsets[0] # choose the first synset\n",
    "            definition = synset.definition()\n",
    "            definitions.append(definition)\n",
    "        else:\n",
    "            definition = \"\"\n",
    "            definitions.append(definition)\n",
    "    return \" \". join(definitions)\n",
    "\n",
    "\n",
    "definitions = target.apply(phrase_expansion)\n",
    "print(definitions.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessing forgi...\n",
      "1       Blessed are the peacemaker for they shall be c...\n",
      "2       I am not conformed to this world I am transfor...\n",
      "3       salad should be prayed with focus and understa...\n",
      "4       And stay in your house and do not display ours...\n",
      "                              ...                        \n",
      "2809    Theres a law protecting unborn eagle but not h...\n",
      "2810    I am 1 in 3 I have had an abortion AbortionOnD...\n",
      "2811    How dare you say my sexual preference is a cho...\n",
      "2812    Equal right for those born that way no right f...\n",
      "2813    POTUS seal his legacy i 12 do win The got agen...\n",
      "Name: Tweet, Length: 2814, dtype: object\n",
      "0       beloved Godhead thank uracil for all of Ur ble...\n",
      "1       bless are the conciliator for they shall beryl...\n",
      "2       iodine americium not conform to this universe ...\n",
      "3       salad should beryllium pray with focus and und...\n",
      "4       And stay inch your house and bash not display ...\n",
      "                              ...                        \n",
      "2809    there angstrom law protect unborn eagle merely...\n",
      "2810    iodine americium one inch three iodine rich_pe...\n",
      "2811    How dare you say my sexual preference be angst...\n",
      "2812    peer right for those Born that manner no right...\n",
      "2813    POTUS sealing_wax his bequest iodine twelve ba...\n",
      "Name: Tweet, Length: 2814, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Query Reformulation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def query_reformulation(tweet):\n",
    "    words = word_tokenize(tweet)\n",
    "    reformulated_words = []\n",
    "    for word in words:\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if synsets:\n",
    "            reformulated_word = synsets[0].lemmas()[0].name()\n",
    "            reformulated_words.append(reformulated_word)\n",
    "        else:\n",
    "            reformulated_words.append(word)\n",
    "    reformulated_query = \" \".join(reformulated_words)\n",
    "    return reformulated_query\n",
    "\n",
    "reformulated_tweets = tweets.apply(query_reformulation)\n",
    "print(tweets)\n",
    "print(reformulated_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([target, extended_target, definitions, tweets, reformulated_tweets, stance], \n",
    "                 keys=[\"target\", \"extended_target\", \"target_definition\", \"tweets\", \"reformulated_tweets\", \"stance\"], axis=1)\n",
    "data.to_csv(\"Datasets/augmented_tweets.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restructure the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'extended_target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vikhy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\vikhy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vikhy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'extended_target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Vikhy\\ASU Coursework\\Semantic Web Mining\\Project\\data-augmentation.ipynb Cell 10\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Vikhy/ASU%20Coursework/Semantic%20Web%20Mining/Project/data-augmentation.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mDatasets/augmented_tweets.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Vikhy/ASU%20Coursework/Semantic%20Web%20Mining/Project/data-augmentation.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m target \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Vikhy/ASU%20Coursework/Semantic%20Web%20Mining/Project/data-augmentation.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m extended_target \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39;49m\u001b[39mextended_target\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Vikhy/ASU%20Coursework/Semantic%20Web%20Mining/Project/data-augmentation.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m target_definition \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtarget_definition\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Vikhy/ASU%20Coursework/Semantic%20Web%20Mining/Project/data-augmentation.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tweets \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtweets\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vikhy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\vikhy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'extended_target'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Datasets/augmented_tweets.csv\")\n",
    "target = data[\"target\"]\n",
    "extended_target = data[\"extended_target\"]\n",
    "target_definition = data[\"target_definition\"]\n",
    "tweets = data[\"tweets\"]\n",
    "reformulated_tweets = data[\"reformulated_tweets\"]\n",
    "\n",
    "stance = data[\"stance\"]\n",
    "\n",
    "\n",
    "tweets_combined = tweets.str.cat(reformulated_tweets, sep=\" can be paraphrased as \")\n",
    "targets_combined = target.str.cat(extended_target, sep=\" which is same as \")\n",
    "targets_definitions_combined = targets_combined.str.cat(target_definition, sep=\" which is defined as \")\n",
    "combined_data = tweets_combined.str.cat(targets_definitions_combined, sep=\" is related to \")\n",
    "\n",
    "\n",
    "\n",
    "final_data = pd.concat([combined_data, stance], keys=[\"Data\", \"Stance\"], axis=1)\n",
    "final_data.to_csv(\"Datasets/restructured_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
